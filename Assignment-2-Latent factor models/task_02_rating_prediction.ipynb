{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Factor Models for  Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to fill out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project task 02: Restaurant recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import time\n",
    "import random\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this task is to recommend restaurants to users based on the rating data in the Yelp dataset. For this, we try to predict the rating a user will give to a restaurant they have not been to yet based on a latent factor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download `ratings.npy` from Piazza ([download link](https://syncandshare.lrz.de/dl/fiKMoxRNusLoFpFHkXXEgvdZ/ratings.npy))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = np.load(\"ratings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101968,   1880,      1],\n",
       "       [101968,    284,      5],\n",
       "       [101968,   1378,      2],\n",
       "       ...,\n",
       "       [ 72452,   2100,      4],\n",
       "       [ 72452,   2050,      5],\n",
       "       [ 74861,   3979,      5]], dtype=uint32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have triplets of (user, restaurant, rating).\n",
    "ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the data into a matrix of dimension [N, D], where N is the number of users and D is the number of restaurants in the dataset.  \n",
    "We **strongly recommend** to load the data as a sparse matrix to avoid out-of-memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the matrix into the variable M\n",
    "M= sp.csr_matrix((ratings[:,2],(ratings[:,0],ratings[:,1])),dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<337867x5899 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 929606 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing step, we recursively remove all users and restaurants with 10 or less ratings. \n",
    "\n",
    "Then, we randomly select 200 data points for the validation and test sets, respectively.\n",
    "\n",
    "After this, we subtract the mean rating for each users to account for this global effect.   \n",
    "**Hint**: Some entries might become zero in this process -- but these entries are different than the 'unknown' zeros in the matrix. Store the indices of which we have data available in a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cold_start_preprocessing(matrix, min_entries):\n",
    "    \"\"\"\n",
    "    Recursively removes rows and columns from the input matrix which have less than min_entries nonzero entries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix      : sp.spmatrix, shape [N, D]\n",
    "                  The input matrix to be preprocessed.\n",
    "    min_entries : int\n",
    "                  Minimum number of nonzero elements per row and column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix      : sp.spmatrix, shape [N', D']\n",
    "                  The pre-processed matrix, where N' <= N and D' <= D\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"Shape before: {}\".format(matrix.shape))\n",
    "    \n",
    "    while True:\n",
    "        matrix = matrix[matrix.getnnz(axis = 1)> min_entries][:, matrix.getnnz(axis = 0)> min_entries]\n",
    "        \n",
    "        if all(matrix.getnnz(axis = 1)> min_entries)== True and all(matrix.getnnz(axis = 0)> min_entries)== True:\n",
    "                break\n",
    "\n",
    "    nnz = matrix>0\n",
    "    \n",
    "    assert (nnz.sum(0).A1 > min_entries).all()\n",
    "    assert (nnz.sum(1).A1 > min_entries).all()\n",
    "    print(\"Shape after: {}\".format(matrix.shape))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift_user_mean(matrix):\n",
    "    \"\"\"\n",
    "    Subtract the mean rating per user from the non-zero elements in the input matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             Input sparse matrix.\n",
    "    Returns\n",
    "    -------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The modified input matrix.\n",
    "    \n",
    "    user_means : np.array, shape [N, 1]\n",
    "                 The mean rating per user that can be used to recover the absolute ratings from the mean-shifted ones.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    user_means=matrix.mean(axis=1)\n",
    "    matrix = matrix.tolil()\n",
    "    for element in range(user_means.shape[0]):\n",
    "        matrix[element,:] = matrix[element,:] - user_means[element]\n",
    "    matrix=sp.csr_matrix(matrix)\n",
    "\n",
    "    assert np.all(np.isclose(matrix.mean(1), 0))\n",
    "    return matrix, user_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(matrix, n_validation, n_test):\n",
    "    \"\"\"\n",
    "    Extract validation and test entries from the input matrix. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix          : sp.spmatrix, shape [N, D]\n",
    "                      The input data matrix.\n",
    "    n_validation    : int\n",
    "                      The number of validation entries to extract.\n",
    "    n_test          : int\n",
    "                      The number of test entries to extract.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix_split    : sp.spmatrix, shape [N, D]\n",
    "                      A copy of the input matrix in which the validation and test entries have been set to zero.\n",
    "    \n",
    "    val_idx         : tuple, shape [2, n_validation]\n",
    "                      The indices of the validation entries.\n",
    "    \n",
    "    test_idx        : tuple, shape [2, n_test]\n",
    "                      The indices of the test entries.\n",
    "    \n",
    "    val_values      : np.array, shape [n_validation, ]\n",
    "                      The values of the input matrix at the validation indices.\n",
    "                      \n",
    "    test_values     : np.array, shape [n_test, ]\n",
    "                      The values of the input matrix at the test indices.\n",
    "\n",
    "    \"\"\"\n",
    "    matrix = matrix.tolil()\n",
    "    \n",
    "    #create row and column indices for the validation set\n",
    "    \n",
    "    row_validation = random.sample(range(0, matrix.shape[0]), n_validation)\n",
    "    column_validation = random.sample(range(0, matrix.shape[1]), n_validation)\n",
    "    \n",
    "    #create row and column indices for the test set (validation set and test set must be disjoint)\n",
    "    \n",
    "    row_test = []\n",
    "    column_test = []\n",
    "    \n",
    "    while len(row_test) < n_test:\n",
    "        x = random.sample(range(0,matrix.shape[0]),1)\n",
    "        y = random.sample(range(0,matrix.shape[1]),1)\n",
    "        if x[0] not in row_validation and y[0] not in column_validation :\n",
    "            row_test.append(x[0])\n",
    "            column_test.append(y[0])\n",
    "        \n",
    "    #save indices as tuples\n",
    "    \n",
    "    indices_test=list(zip(row_test, column_test))\n",
    "    indices_validate=list(zip(row_validation, column_validation))\n",
    "    \n",
    "    #create val_idx and val_values\n",
    "    \n",
    "    validation_values=[]\n",
    "    for element in indices_validate:\n",
    "        validation_values.append(matrix[element[0],element[1]])\n",
    "\n",
    "    val_idx=np.transpose(np.array(indices_validate))\n",
    "    val_values=np.array(validation_values)\n",
    "    \n",
    "    #create test_idx and test_values\n",
    "    test_values=[]\n",
    "    for element in indices_test:\n",
    "        test_values.append(matrix[element[0],element[1]])\n",
    "\n",
    "    test_idx=np.transpose(np.array(indices_test))\n",
    "    test_values=np.array(test_values)\n",
    "\n",
    "    matrix[row_validation,column_validation]=0\n",
    "    matrix[row_test,column_test]=0\n",
    "    matrix_split=sp.csr_matrix(matrix)\n",
    "\n",
    "    print('Shape of matrix_split is ',matrix_split.shape)\n",
    "    print('Shape of val_idx is ',val_idx.shape)\n",
    "    print('Shape of test_idx is ',test_idx.shape)\n",
    "    print('Shape of val_values is ',val_values.shape)\n",
    "    print('Shape of test_values is ',test_values.shape)\n",
    "    \n",
    "    return matrix_split, val_idx, test_idx, val_values, test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (337867, 5899)\n",
      "Shape after: (11275, 3531)\n"
     ]
    }
   ],
   "source": [
    "M = cold_start_preprocessing(M, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix_split is  (11275, 3531)\n",
      "Shape of val_idx is  (2, 200)\n",
      "Shape of test_idx is  (2, 200)\n",
      "Shape of val_values is  (200,)\n",
      "Shape of test_values is  (200,)\n"
     ]
    }
   ],
   "source": [
    "n_validation = 200\n",
    "n_test = 200\n",
    "# Split data\n",
    "M_train, val_idx, test_idx, val_values, test_values = split_data(M, n_validation, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store away the nonzero indices of M before subtracting the row means.\n",
    "# nonzero_indices has three arrays: row indices, column indices, and values of the nonzero matrix entries.\n",
    "nonzero_indices = sp.find(M_train)\n",
    "\n",
    "# Remove user means.\n",
    "M_shifted, user_means = shift_user_mean(M_train)\n",
    "\n",
    "#Apply the same shift to the validation and test data.\n",
    "val_values_shifted = val_values-user_means[val_idx[0]]\n",
    "test_values_shifted = test_values-user_means[test_idx[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Alternating optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, we will approach the problem via alternating optimization, as learned in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_Q_P(matrix, k, init='random'):\n",
    "    \"\"\"\n",
    "    Initialize the matrices Q and P for a latent factor model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The matrix to be factorized.\n",
    "    k      : int\n",
    "             The number of latent dimensions.\n",
    "    init   : str in ['svd', 'random'], default: 'random'\n",
    "             The initialization strategy. 'svd' means that we use SVD to initialize P and Q, 'random' means we initialize\n",
    "             the entries in P and Q randomly in the interval [0, 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : np.array, shape [N, k]\n",
    "        The initialized matrix Q of a latent factor model.\n",
    "\n",
    "    P : np.array, shape [k, D]\n",
    "        The initialized matrix P of a latent factor model.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if init == 'svd':\n",
    "        Q,_,P = svds(matrix,k)\n",
    "    elif init == 'random':\n",
    "        Q = np.random.uniform(0,1,size = (matrix.shape[0],k))\n",
    "        P = np.random.uniform(0,1,size = (k,matrix.shape[1]))\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "    assert Q.shape == (matrix.shape[0], k)\n",
    "    assert P.shape == (k, matrix.shape[1])\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "Q,P=initialize_Q_P(M_train, k, init='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11275, 1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  3005624.771934244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dbogu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is:  4410562.026130993\n",
      "Loss is:  4410563.999970826\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-ea85943bc717>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mM_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow_index_qi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrelevant_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mp_new\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrelevant_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mmodel_q\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mq_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_q\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow_index_qi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_new\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m         \"\"\"\n\u001b[1;32m--> 665\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRidge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    497\u001b[0m                 \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                 \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m                 return_intercept=True)\n\u001b[0m\u001b[0;32m    500\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0my_offset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py\u001b[0m in \u001b[0;36mridge_regression\u001b[1;34m(X, y, alpha, sample_weight, solver, max_iter, tol, verbose, random_state, return_n_iter, return_intercept)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                 \u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m                 is_saga=solver == 'saga')\n\u001b[0m\u001b[0;32m    428\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreturn_intercept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[0mcoef\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py\u001b[0m in \u001b[0;36msag_solver\u001b[1;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[0;32m    321\u001b[0m                             \u001b[0mintercept_decay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                             \u001b[0mis_saga\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m                             verbose)\n\u001b[0m\u001b[0;32m    324\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         warnings.warn(\"The max_iter was reached which means \"\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_p=Ridge(alpha=1.0, max_iter =4000)\n",
    "model_q=Ridge(alpha=1.0, max_iter =4000)\n",
    "\n",
    "#optimizing Q over P\n",
    "for _ in range(10):\n",
    "    subtracted_M = M_train-np.dot(Q,P)\n",
    "    M_mask = M_train>0\n",
    "    subtracted_M = sp.csr_matrix(subtracted_M)\n",
    "    result=M_mask.multiply(subtracted_M)\n",
    "    result=result.power(2)\n",
    "    summed_elements=result.sum()\n",
    "    #print(summed_elements)\n",
    "    #summed_elements=np.sqrt(summed_elements)\n",
    "    #summed_elements=summed_elements/np.sum(M_mask)\n",
    "    print('Loss is: ',summed_elements)\n",
    "    \n",
    "    for row_index_qi in range(M_train.shape[0]):\n",
    "        boolean_mask=M_train[row_index_qi,:]>0\n",
    "        relevant_columns=sp.find(boolean_mask)[1] \n",
    "        y=M_train[row_index_qi,relevant_columns]\n",
    "        p_new=P[:,relevant_columns]\n",
    "        model_q.fit(y.T,p_new.T)\n",
    "        q_new = model_q.coef_\n",
    "        Q[row_index_qi,:] = np.reshape(q_new,(k))\n",
    "\n",
    "    for column_index_px in range(M_train.shape[1]):\n",
    "        boolean_mask=M_train[:, column_index_px]>0\n",
    "        relevant_rows=sp.find(boolean_mask)[0] \n",
    "        y=M_train[relevant_rows,column_index_px]\n",
    "        q_new=Q[relevant_rows,:]\n",
    "        model_p.fit(y,q_new)\n",
    "        p_new = model_p.coef_\n",
    "        P[:,column_index_px] = np.reshape(p_new,(k))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1092234.1986212193\n",
      "Loss is:  0.0036626483852272656\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subtracted_M = M_train-np.dot(Q,P)\n",
    "M_mask = M_train>0\n",
    "subtracted_M = sp.csr_matrix(subtracted_M)\n",
    "result=M_mask.multiply(subtracted_M)\n",
    "result=result.power(2)\n",
    "summed_elements=result.sum()\n",
    "print(summed_elements)\n",
    "summed_elements=np.sqrt(summed_elements)\n",
    "summed_elements=summed_elements/np.sum(M_mask)\n",
    "print('Loss is: ',summed_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_mask = M_train>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtracted_M = sp.csr_matrix(subtracted_M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=M_mask.multiply(subtracted_M)\n",
    "result=result.power(2)\n",
    "result.todense()\n",
    "summed_elements=np.sum(result)\n",
    "summed_elements=np.sqrt(summed_elements)\n",
    "summed_elements=summed_elements/np.sum(M_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007249721377532491"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def latent_factor_alternating_optimization(M, non_zero_idx, k, val_idx, val_values,\n",
    "                                           reg_lambda, max_steps=100, init='random',\n",
    "                                           log_every=1, patience=10, eval_every=1):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using alternating optimization. Training is done via patience,\n",
    "    i.e. we stop training after we observe no improvement on the validation loss for a certain\n",
    "    amount of training steps. We then return the best values for Q and P oberved during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M                 : sp.spmatrix, shape [N, D]\n",
    "                        The input matrix to be factorized.\n",
    "                      \n",
    "    non_zero_idx      : np.array, shape [nnz, 2]\n",
    "                        The indices of the non-zero entries of the un-shifted matrix to be factorized. \n",
    "                        nnz refers to the number of non-zero entries. Note that this may be different\n",
    "                        from the number of non-zero entries in the input matrix M, e.g. in the case\n",
    "                        that all ratings by a user have the same value.\n",
    "    \n",
    "    k                 : int\n",
    "                        The latent factor dimension.\n",
    "    \n",
    "    val_idx           : tuple, shape [2, n_validation]\n",
    "                        Tuple of the validation set indices.\n",
    "                        n_validation refers to the size of the validation set.\n",
    "                      \n",
    "    val_values        : np.array, shape [n_validation, ]\n",
    "                        The values in the validation set.\n",
    "                      \n",
    "    reg_lambda        : float\n",
    "                        The regularization strength.\n",
    "                      \n",
    "    max_steps         : int, optional, default: 100\n",
    "                        Maximum number of training steps. Note that we will stop early if we observe\n",
    "                        no improvement on the validation error for a specified number of steps\n",
    "                        (see \"patience\" for details).\n",
    "                      \n",
    "    init              : str in ['random', 'svd'], default 'random'\n",
    "                        The initialization strategy for P and Q. See function initialize_Q_P for details.\n",
    "    \n",
    "    log_every         : int, optional, default: 1\n",
    "                        Log the training status every X iterations.\n",
    "                    \n",
    "    patience          : int, optional, default: 10\n",
    "                        Stop training after we observe no improvement of the validation loss for X evaluation\n",
    "                        iterations (see eval_every for details). After we stop training, we restore the best \n",
    "                        observed values for Q and P (based on the validation loss) and return them.\n",
    "                      \n",
    "    eval_every        : int, optional, default: 1\n",
    "                        Evaluate the training and validation loss every X steps. If we observe no improvement\n",
    "                        of the validation error, we decrease our patience by 1, else we reset it to *patience*.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_Q            : np.array, shape [N, k]\n",
    "                        Best value for Q (based on validation loss) observed during training\n",
    "                      \n",
    "    best_P            : np.array, shape [k, D]\n",
    "                        Best value for P (based on validation loss) observed during training\n",
    "                      \n",
    "    validation_losses : list of floats\n",
    "                        Validation loss for every evaluation iteration, can be used for plotting the validation\n",
    "                        loss over time.\n",
    "                        \n",
    "    train_losses      : list of floats\n",
    "                        Training loss for every evaluation iteration, can be used for plotting the training\n",
    "                        loss over time.                     \n",
    "    \n",
    "    converged_after   : int\n",
    "                        it - patience*eval_every, where it is the iteration in which patience hits 0,\n",
    "                        or -1 if we hit max_steps before converging. \n",
    "\n",
    "    \"\"\"\n",
    "    Q,P=initialize_Q_P(M, k, init='random')\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the latent factor model with alternating optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Learn the optimal $P$ and $Q$ using alternating optimization. That is, during each iteration you first update $Q$ while having $P$ fixed and then vice versa. Run the alternating optimization algorithm with $k=100$ and $\\lambda=1$. Plot the training and validation losses over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_a, P_a, val_l_a, tr_l_a, conv_a = latent_factor_alternating_optimization(M_shifted, nonzero_indices, \n",
    "                                                                           k=100, val_idx=val_idx,\n",
    "                                                                           val_values=val_values_shifted, \n",
    "                                                                           reg_lambda=1, init='random',\n",
    "                                                                           max_steps=100, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) (**Optional**): Try some different latent dimensions $k$ in the range [5, 100]. What do you observe (convergence time, final training/validation losses)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_a_2, P_a_2, val_l_a_2, tr_l_a_2, conv_a_2 = latent_factor_alternating_optimization(M_shifted, nonzero_indices, \n",
    "                                                                           k=20, val_idx=val_idx,\n",
    "                                                                           val_values=val_values_shifted, \n",
    "                                                                           reg_lambda=0.1, init='random',\n",
    "                                                                           max_steps=100, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent factorization using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use gradient descent to factorize our ratings matrix. We will try both (mini-) batch and stochastic gradient descent. You can use the following equations for your implementation.\n",
    "\n",
    "Recall that the objective function (loss) we wanted to optimize was:\n",
    "$$\n",
    "\\mathcal{L} = \\min_{P, Q} \\sum_{(x, i) \\in W} (r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)^2 + \\lambda_1\\sum_x{\\left\\lVert \\mathbf{p}_x  \\right\\rVert}^2 + \\lambda_2\\sum_i {\\left\\lVert\\mathbf{q}_i  \\right\\rVert}^2\n",
    "$$\n",
    "\n",
    "where $W$ is the set of $(x, i)$ pairs for which $r_{xi}$ is known (in this case our known play counts). Here we have also introduced two regularization terms to help us with overfitting where $\\lambda_1$ and $\\lambda_2$ are hyper-parameters that control the strength of the regularization.\n",
    "\n",
    "Naturally optimizing with gradient descent involves computing the gradient of the loss function $\\mathcal{L}$ w.r.t. to the parameters. To help you solve the task we provide the following:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial ((r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)^2)}{\\partial \\mathbf{p}_x} = -2(r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)\\mathbf{q}_i\\;, ~~~\n",
    "\\frac{\\partial ((r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)^2)}{\\partial \\mathbf{q}_i} = -2(r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)\\mathbf{p}_x \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial(\\lambda_1{\\left\\lVert \\mathbf{p}_x \\right\\rVert}^2)}{\\partial \\mathbf{p}_x} = 2 \\lambda_1 \\mathbf{p_x} \\;, ~~~\n",
    "\\frac{\\partial(\\lambda_2{\\left\\lVert \\mathbf{q}_i \\right\\rVert}^2)}{\\partial \\mathbf{q}_i} = 2 \\lambda_2 \\mathbf{q_i}\n",
    "$$\n",
    "\n",
    "**Hint**: You have to carefully consider how to combine the given partial gradients depending\n",
    "on which variants of gradient descent you are using.  \n",
    "**Hint 2**: It may be useful to scale the updates to $P$ and $Q$ by $\\frac{1}{batch\\_size}$ (in the case of full-sweep updates, this would be $\\frac{1}{n\\_users}$ for $Q$ and $\\frac{1}{n\\_restaurants}$ for $P$).\n",
    "\n",
    "\n",
    "For each of the gradients descent variants you try report and compare the following:\n",
    "* How many iterations do you need for convergence.\n",
    "* Plot the loss (y axis) for each iteration (x axis).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def latent_factor_gradient_descent(M, non_zero_idx, k, val_idx, val_values, \n",
    "                                   reg_lambda, learning_rate, batch_size=-1,\n",
    "                                   max_steps=50000, init='random',\n",
    "                                   log_every=1000, patience=20,\n",
    "                                   eval_every=50):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using gradient descent. Training is done via patience,\n",
    "    i.e. we stop training after we observe no improvement on the validation loss for a certain\n",
    "    amount of training steps. We then return the best values for Q and P oberved during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M                 : sp.spmatrix, shape [N, D]\n",
    "                        The input matrix to be factorized.\n",
    "                      \n",
    "    non_zero_idx      : np.array, shape [nnz, 2]\n",
    "                        The indices of the non-zero entries of the un-shifted matrix to be factorized. \n",
    "                        nnz refers to the number of non-zero entries. Note that this may be different\n",
    "                        from the number of non-zero entries in the input matrix M, e.g. in the case\n",
    "                        that all ratings by a user have the same value.\n",
    "    \n",
    "    k                 : int\n",
    "                        The latent factor dimension.\n",
    "    \n",
    "    val_idx           : tuple, shape [2, n_validation]\n",
    "                        Tuple of the validation set indices.\n",
    "                        n_validation refers to the size of the validation set.\n",
    "                      \n",
    "    val_values        : np.array, shape [n_validation, ]\n",
    "                        The values in the validation set.\n",
    "                      \n",
    "    reg_lambda        : float\n",
    "                        The regularization strength.\n",
    "\n",
    "    learning_rate     : float\n",
    "                        Step size of the gradient descent updates.\n",
    "                        \n",
    "    batch_size        : int, optional, default: -1\n",
    "                        (Mini-) batch size. -1 means we perform standard full-sweep gradient descent.\n",
    "                        If the batch size is >0, use mini batches of this given size.\n",
    "                        \n",
    "    max_steps         : int, optional, default: 100\n",
    "                        Maximum number of training steps. Note that we will stop early if we observe\n",
    "                        no improvement on the validation error for a specified number of steps\n",
    "                        (see \"patience\" for details).\n",
    "                      \n",
    "    init              : str in ['random', 'svd'], default 'random'\n",
    "                        The initialization strategy for P and Q. See function initialize_Q_P for details.\n",
    "    \n",
    "    log_every         : int, optional, default: 1\n",
    "                        Log the training status every X iterations.\n",
    "                    \n",
    "    patience          : int, optional, default: 10\n",
    "                        Stop training after we observe no improvement of the validation loss for X evaluation\n",
    "                        iterations (see eval_every for details). After we stop training, we restore the best \n",
    "                        observed values for Q and P (based on the validation loss) and return them.\n",
    "                      \n",
    "    eval_every        : int, optional, default: 1\n",
    "                        Evaluate the training and validation loss every X steps. If we observe no improvement\n",
    "                        of the validation error, we decrease our patience by 1, else we reset it to *patience*.\n",
    "                        \n",
    "    Returns\n",
    "    -------\n",
    "    best_Q            : np.array, shape [N, k]\n",
    "                        Best value for Q (based on validation loss) observed during training\n",
    "                      \n",
    "    best_P            : np.array, shape [k, D]\n",
    "                        Best value for P (based on validation loss) observed during training\n",
    "                      \n",
    "    validation_losses : list of floats\n",
    "                        Validation loss for every evaluation iteration, can be used for plotting the validation\n",
    "                        loss over time.\n",
    "                        \n",
    "    train_losses      : list of floats\n",
    "                        Training loss for every evaluation iteration, can be used for plotting the training\n",
    "                        loss over time.                     \n",
    "    \n",
    "    converged_after   : int\n",
    "                        it - patience*eval_every, where it is the iteration in which patience hits 0,\n",
    "                        or -1 if we hit max_steps before converging. \n",
    "\n",
    "    \"\"\"\n",
    "       \n",
    "    ### YOUR CODE HERE ###\n",
    "            \n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the latent factor model with alternating optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Learn the optimal $P$ and $Q$ using standard gradient descent. That is, during each iteration you have to use all of the training examples and update $Q$ and $P$ for all users and songs at once. Try the algorithm with $k=30$, $\\lambda=1$, and learning rate of 0.1. Initialize $Q$ and $P$ with SVD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_g_sweep, P_g_sweep, val_l_g_sweep, tr_l_g_sweep, conv_g_sweep =  latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                                   k=30, val_idx=val_idx,\n",
    "                                                                                                   val_values=val_values_shifted, \n",
    "                                                                                                   reg_lambda=1, learning_rate=1e-1,\n",
    "                                                                                                   init='svd', batch_size=-1,\n",
    "                                                                                                   max_steps=10000, log_every=20, \n",
    "                                                                                                   eval_every=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Learn the optimal $P$ and $Q$ using the original stochastic gradient descent (mini-batches of size 1). That is, during each iteration you sample a single random training example $r_{xi}$ and update only the respective affected parameters $\\mathbf{p_x}$ and $\\mathbf{q}_i$. Set the learning rate to 0.01 and keep the other parameters as in a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_g_st, P_g_st, val_l_g_st, tr_l_g_st, conv_g_st = latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                   k=30, val_idx=val_idx,\n",
    "                                                                                   val_values=val_values_shifted, \n",
    "                                                                                   reg_lambda=1, learning_rate=1e-2,\n",
    "                                                                                   init='svd', batch_size=1,\n",
    "                                                                                   max_steps=20000, log_every=500, \n",
    "                                                                                   eval_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) (**Optional**) Learn the optimal $P$ and $Q$ similarly to b) this time using larger mini-batches of size $S$, e.g. 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_g_mb, P_g_mb, val_l_g_mb, tr_l_g_mb, conv_g_mb = latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                   k=30, val_idx=val_idx,\n",
    "                                                                                   val_values=val_values_shifted, \n",
    "                                                                                   reg_lambda=1, learning_rate=1e-1,\n",
    "                                                                                   init='svd', batch_size=32,\n",
    "                                                                                   max_steps=10000, log_every=100, \n",
    "                                                                                   eval_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models are often heavily dependent on the hyperparameter settings, e.g. the learning rate. Here, we will try a simple random search to find good values of the latent factor dimension $k$, the batch size, learning rate, and regularization.  \n",
    "\n",
    "### Tasks:\n",
    "\n",
    "Perform a hyperparameter search to find good values for the batch size, lambda, learning rate, and latent dimension. \n",
    "\n",
    "* For the batch size, evaluate all values in [1, 32, 512, -1] (-1 corresponds to full-sweep gradient descent).\n",
    "* For $\\lambda$, randomly sample three values in the interval [0, 1).\n",
    "* For the learning rate, evaluate all values in [1, 0.1, 0.01].\n",
    "* For the latent dimension, uniformly sample three values in the interval [5,30].\n",
    "\n",
    "Perform an exhaustive search among all combinations of these values;\n",
    "\n",
    "**Hint**: This may take a while to compute. **You don't have to wait for all the models to train** -- simply use \"dummy\" code instead of actual model training (or let it train, e.g., for only one iteration) if you don't want to wait. Note that the signature of this dummy code has to match the function 'latent_factor_gradient_descent' so that we could simply plug in the actual function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parameter_search(M_train, val_idx, val_values):\n",
    "    \"\"\"\n",
    "    Hyperparameter search using random search.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    M_train     : sp.spmatrix, shape [N, D]\n",
    "                  Input sparse matrix where the user means have not\n",
    "                  been subtracted yet. \n",
    "                  \n",
    "    val_idx     : tuple, shape [2, n_validation]\n",
    "                  The indices used for validation, where n_validation\n",
    "                  is the size of the validation set.\n",
    "                  \n",
    "    val_values  : np.array, shape [n_validation, ]\n",
    "                  Validation set values, where n_validation is the\n",
    "                  size of the validation set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_conf   : tuple, (batch_size, lambda, learning_rate, latent_dimension)\n",
    "                  The best-performing hyperparameters.\n",
    "                  \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "        \n",
    "    print(\"Best configuration is {}\").format(best_conf)\n",
    "    return best_conf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_configuration = parameter_search(M_train, val_idx, val_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output the best hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison of gradient descent and alternating optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the latent factor model with both alternating optimization and gradient descent, we now compare their results on the training, validation, and test set.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* Compare the root mean square errors (RMSE) for the training, validation, and test sets different settings of $k$ for both alternating optimization and gradient descent. What do you observe?\n",
    "* Compare the test RMSE for the alternating optimization model and the gradient descent model. Which performs better?\n",
    "* Plot the predicted ratings\n",
    "\n",
    "**Hint**: The output values and plots below are the ones we got when testing this sheet. Yours may be different, but if your validation or test RMSE values are larger than 1.5 or 2, it is likely that you have a bug in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots: Prediction vs. ground truth ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
